---
title: "Tidy Text"
author: "Anish Yakkala, Lemar Popal, Brooke Hanna, Michal Golovanevsky"
date: "3/7/2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)

# This is a test comment
```


## Tidy Text Philosophy

## Tokenization

```{r message=FALSE, warning=FALSE}
polyrating <- read_csv("polyrating.csv") %>% drop_na()
```
```{r}
tokens <- polyrating %>%
  unnest_tokens(word,review) %>%
  select(word,everything())

tokens
```

```{r message=FALSE, warning=FALSE}
data(stop_words)

tokens <- tokens %>%
  anti_join(stop_words)
```

```{r}
token_count <- tokens %>%
  count(word, sort = TRUE)

token_count
```

```{r}
tokens %>%
  count(word, sort = TRUE) %>%
  filter(n > 17500) %>%
  mutate(word = reorder(word, n )) %>%
  ggplot(aes(word, n )) + geom_col() + xlab(NULL) + coord_flip()
```

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```

```{r message=FALSE, warning=FALSE}
library(stringr)
library(lubridate)

nrc_joy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")
```

```{r}
tokens %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

```{r cosam_sentiment, message=FALSE}
tokens %>%
  mutate(date = parse_date_time(date,"%m%y")) %>%
  inner_join(get_sentiments("bing")) %>%
  filter(subject %in% c("MATH","STAT","CHEM","PHYS","BIO","LS")) %>%
  count(date, subject, sentiment) %>%
  spread(sentiment,n,fill=0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(date,sentiment,fill=subject)) + geom_col(show.legend = FALSE) +
  facet_wrap(~subject) + ggtitle("COSAM Sentiment Over Time")
```

## Sentiment Analysis

```{r}
bing_word_counts <- tokens %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```
```{r bing_comparison, message=FALSE}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```
 

```{r}
custom_stop_words <- bind_rows(tibble(word = c("class"), 
                                          lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

```{r message=FALSE}
library(wordcloud)

tokens %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r message=FALSE}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

bingpositive <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")

wordcounts <- tokens %>%
  group_by(subject) %>%
  summarize(words = n())

tokens %>%
  semi_join(bingnegative) %>%
  group_by(subject) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("subject")) %>%
  mutate(ratio = negativewords/words) %>%
  ungroup() %>%
  arrange(desc(ratio))
```

```{r message=FALSE}
tokens %>%
  semi_join(bingpositive) %>%
  group_by(subject) %>%
  summarize(positivewords = n()) %>%
  left_join(wordcounts, by = c("subject")) %>%
  mutate(ratio = positivewords/words) %>%
  ungroup() %>%
  arrange(desc(ratio))
```

## TF-IDF

```{r message=FALSE}
review_words <- tokens %>%
  count(subject, word, sort = TRUE) %>%
  ungroup()

total_words <- review_words %>%
  group_by(subject) %>%
  summarize(total = sum(n))

review_words <- left_join(review_words, total_words)

review_words
```
```{r term_freq_graph,message=FALSE, warning=FALSE}
review_words %>%
  filter(subject %in% c("MATH","STAT","CHEM","PHYS","BIO","LS")) %>%
  ggplot(aes(n/total, fill = subject)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~subject, ncol = 2, scales = "free_y")
```

```{r}
freq_by_rank <- review_words %>%
                group_by(subject) %>%
                mutate(rank = row_number(),
                       term_freq = n/total)

freq_by_rank
```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, term_freq, color = subject)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(term_freq) ~ log10(rank), data = rank_subset)
```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, term_freq, color = subject)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

```{r}
review_tf_words <- review_words %>%
  bind_tf_idf(word, subject, n)

review_tf_words
```

```{r}
review_tf_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

```{r cosam_tf_graph, message=FALSE}
review_tf_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  filter(subject %in% c("MATH","STAT","CHEM","PHYS","BIO","LS")) %>%
  group_by(subject) %>% 
  top_n(10) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = subject)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~subject, ncol = 2, scales = "free") +
  coord_flip()
```

## N-grams

```{r}
poly_bigrams <- polyrating %>%
  unnest_tokens(bigram,review,token="ngrams",n=2) %>%
  select(bigram,everything())

poly_bigrams
```

```{r}
poly_bigrams %>%
  count(bigram, sort = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(stringr)

names <- tokens %>%
  select(prof_name) %>%
  distinct(prof_name) %>%
  separate(prof_name, c("last", "first"), sep = ",") %>%
  gather() %>%
  select(value) %>%
  mutate(value = str_to_lower(value)) %>%
  pull(value)


custom_stop_words <-stop_words %>%
  filter(!(word %in% c("her","she","he","his"))) %>%
  add_row(word = names,lexicon = "SMART")
```

```{r}
bigrams_separated <- poly_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% custom_stop_words$word) %>%
  filter(!word2 %in% custom_stop_words$word)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```


```{r}
trigrams_filtered <- polyrating %>%
  unnest_tokens(trigram, review, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in%  custom_stop_words$word,
         !word2 %in% custom_stop_words$word,
         !word3 %in% custom_stop_words$word)

trigrams_filtered
```

```{r}
bigrams_filtered %>%
  filter(word1 == "she" | word2 == "she") %>%
  count(subject,word1,word2, sort = TRUE)
```
```{r}
bigrams_filtered %>%
  filter(word1 == "he" | word2 == "he") %>%
  count(subject,word1,word2, sort = TRUE)
```

```{r}
trigrams_filtered %>%
  filter(word1 == "she" | word2 == "she"| word3 == "she") %>%
  count(subject,word1,word2,word3, sort = TRUE)
```

```{r}
trigrams_filtered %>%
  filter(word1 == "he" | word2 == "he"| word3 == "he") %>%
  count(subject,word1,word2,word3, sort = TRUE)
```

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(subject, bigram) %>%
  bind_tf_idf(bigram, subject, n) %>%
  arrange(desc(tf_idf))
```

```{r cosam_bi_graphs, message=FALSE, warning=FALSE}
bigram_tf_idf %>%
  filter(subject %in% c("MATH","STAT","CHEM","PHYS","BIO","LS")) %>%
  group_by(subject) %>% 
  top_n(5) %>% 
  mutate(bigram = fct_reorder(bigram,n)) %>%
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = subject)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~subject, ncol = 2, scales = "free") +
  coord_flip()

bigram_tf_idf %>%
  filter(subject %in% c("MATH","STAT","CHEM","PHYS","BIO","LS")) %>%
  group_by(subject) %>% 
  top_n(5,tf) %>% 
  mutate(bigram = fct_reorder(bigram,n)) %>%
  ungroup() %>%
  ggplot(aes(bigram, tf, fill = subject)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf") +
  facet_wrap(~subject, ncol = 2, scales = "free") +
  coord_flip()

  
```
```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

```{r}
AFINN <- get_sentiments("afinn")

AFINN
```

```{r}
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

not_words
```

```{r}
not_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, -n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```

```{r}
negation_words <- c("not", "no", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()
```


```{r afinn_graph}
negated_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, -n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"negation words\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip() + facet_wrap(~word1,scales="free")
```

```{r}
year_counts <- polyrating %>%
  unnest_tokens(word,review) %>%
  select(word,everything()) %>%
  count(date, word) %>%
  complete(date, word, fill = list(n = 0)) 
  
year_counts
```

```{r words_over_time, message=FALSE, warning=FALSE}
year_totals <- year_counts %>%
  group_by(date) %>%
  summarize(year_total = sum(n))

year_counts %>%
  left_join(year_totals, by = "date") %>%
  mutate(date = parse_date_time(date,"%m%y")) %>%
  filter(word %in% c("terrible", "he", "confusing", "she", "easy", "hard")) %>%
  ggplot(aes(date, n / year_total)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~ word, scales = "free_y") +
  scale_y_continuous(labels = scales::percent_format()) +
  ylab("% frequency of word in review")
```





